{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN"
      ],
      "metadata": {
        "id": "HlT0DFWvciHS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pDkhwpAG_Oor"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define the RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Define a fully connected layer for output\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Pass input through RNN\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # hn: tensor of shape (num_layers, batch_size, hidden_size)\n",
        "        out, hn = self.rnn(x, h0)\n",
        "\n",
        "        # Apply FC layer to the output of each time step for sequence-to-sequence prediction\n",
        "        # Reshape 'out' from (batch_size, seq_length, hidden_size) to (batch_size * seq_length, hidden_size)\n",
        "        # Apply FC layer to get (batch_size * seq_length, output_size)\n",
        "        # Reshape back to (batch_size, seq_length, output_size) if needed, but not for CrossEntropyLoss\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Character-level prediction\n",
        "text = \"hello world\"\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
        "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "input_seq = [char_to_int[ch] for ch in text[:-1]]\n",
        "target_seq = [char_to_int[ch] for ch in text[1:]]\n",
        "\n",
        "# One-hot encode input\n",
        "input_one_hot = torch.zeros(len(input_seq), len(chars))\n",
        "for i, char_idx in enumerate(input_seq):\n",
        "    input_one_hot[i, char_idx] = 1\n",
        "\n",
        "# Reshape for batch_first=True: (batch_size, seq_length, input_size)\n",
        "input_tensor = input_one_hot.unsqueeze(0)\n",
        "target_tensor = torch.tensor(target_seq).unsqueeze(0)"
      ],
      "metadata": {
        "id": "MXIf5REjAASx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "input_size = len(chars)\n",
        "hidden_size = 128\n",
        "output_size = len(chars)\n",
        "num_layers = 1\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleRNN(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop (simplified)\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input_tensor) # output shape: (batch_size, seq_length, output_size)\n",
        "\n",
        "    # Reshape output for CrossEntropyLoss: (N, C) where N is batch_size * seq_length\n",
        "    # Target should be (N,)\n",
        "    loss = criterion(output.reshape(-1, output_size), target_tensor.squeeze(0))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Prediction (example)\n",
        "with torch.no_grad():\n",
        "    predicted_output = model(input_tensor) # shape: (batch_size, seq_length, output_size)\n",
        "\n",
        "    # For sequence-to-sequence, get the argmax along the output_size dimension for each time step\n",
        "    predicted_char_indices = torch.argmax(predicted_output, dim=2).squeeze(0) # shape: (seq_length,)\n",
        "    predicted_chars = [int_to_char[idx.item()] for idx in predicted_char_indices]\n",
        "    print(\"Predicted sequence:\", \"\".join(predicted_chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMam_tzKACvl",
        "outputId": "7eed2fd8-6915-4d2a-93d7-c032db47ad49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.1196\n",
            "Epoch [20/100], Loss: 0.0041\n",
            "Epoch [30/100], Loss: 0.0007\n",
            "Epoch [40/100], Loss: 0.0003\n",
            "Epoch [50/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [70/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Predicted sequence: ello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional RNN"
      ],
      "metadata": {
        "id": "yB98WzwOc0Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "features = 2000\n",
        "max_len = 50\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=features)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssBTASg6cz3I",
        "outputId": "fe195ec1-9d90-4bb4-81cb-e392a7e94ec2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.terminal.embed import EmbeddedMagics\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Bidirectional,SimpleRNN,Dense\n",
        "\n",
        "embedding_dim = 128\n",
        "hidden_units = 64\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(features, embedding_dim, input_length=max_len))\n",
        "\n",
        "model.add(Bidirectional(SimpleRNN(hidden_units)))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6vzCQn2idQQ3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDB0j1gAdQMh",
        "outputId": "9d064910-e5c3-4346-ebb2-aabe43a5988e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 35ms/step - accuracy: 0.6213 - loss: 0.6246 - val_accuracy: 0.7777 - val_loss: 0.4821\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 35ms/step - accuracy: 0.8000 - loss: 0.4380 - val_accuracy: 0.7842 - val_loss: 0.4560\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 35ms/step - accuracy: 0.8547 - loss: 0.3429 - val_accuracy: 0.7665 - val_loss: 0.5014\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 34ms/step - accuracy: 0.9037 - loss: 0.2458 - val_accuracy: 0.7718 - val_loss: 0.5800\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9449 - loss: 0.1536 - val_accuracy: 0.7442 - val_loss: 0.7218\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e4bf3232180>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UWtr3ZEdQFw",
        "outputId": "f843e39c-8c3f-4af2-a3ef-f3541e5abddb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.7316\n",
            "Test accuracy: 0.744159996509552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z_tsFLeerwl",
        "outputId": "f8b753b3-66fd-41de-9d1d-627a81e5aa5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 21ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.67      0.72     12500\n",
            "    Positive       0.71      0.82      0.76     12500\n",
            "\n",
            "    accuracy                           0.74     25000\n",
            "   macro avg       0.75      0.74      0.74     25000\n",
            "weighted avg       0.75      0.74      0.74     25000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}